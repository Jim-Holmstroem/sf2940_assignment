\documentclass[a4paper,twoside=false,abstract=false,numbers=noenddot,
titlepage=false,headings=small,parskip=half,version=last]{scrartcl}
\usepackage{lib/header}
\usepackage{lib/probability}
\begin{document}
\generateheader{Assignment 1}

%------------------------------------------------------------------------------

\begin{exercise}{1: 1.12.3.10} {\bf Intersection of sigma algebras}   \\
    $\FF_1$ and $\FF_2$ are two sigma algebras of subsets of $\Omega$. Show
    that
    \begin{equation}
        \FF_1 \cap \FF_2
    \end{equation}
    is a sigma algebra of subsets of $\Omega$.
\end{exercise}
\begin{solution}
    \begin{definition}[Sigma algebra]
        $\FF \subseteq \PP(\Omega)$ is called a \textbf{sigma
        algebra} if it satisfies.
        \begin{equation}
            \label{eq:nonempty}
            \Omega \in \FF
        \end{equation}
        \begin{equation}
            \label{eq:complement}
            A\in \FF \Rightarrow A^c \in \FF
        \end{equation}
        \begin{equation}
            \label{eq:union}
            A, B\in \FF \Rightarrow A\cup B \in \FF
        \end{equation}
        In other words, non-empty\eqref{eq:nonempty} and has closure under both
        complement\eqref{eq:complement} and union\eqref{eq:union}.
    \end{definition}

    Verify the sigma algebra axioms with the universe $\Omega$.\\

    By an alternative definition of intersection we have (which holds for any
    set)
    \begin{equation}
        \label{eq:altintersection}
        A \in \FF_1\wedge A \in \FF_2 = A \in \FF_1, \FF_2 \Leftrightarrow A \in \FF_1 \cap \FF_2
    \end{equation}

    Axiom \ref{eq:nonempty}:
    Just applying \eqref{eq:altintersection} directly and using the fact that
    $\FF_1$ and $\FF_2$ is sigma algebras.
    \begin{equation}
        \Omega \in \FF_1, \FF_2 \Rightarrow \Omega \in \FF_1 \cap \FF_2
    \end{equation}

    Axiom \ref{eq:complement}:
    Just applying \eqref{eq:altintersection} directly and using the fact that
    $\FF_1$ and $\FF_2$ is sigma algebras.
    \begin{equation}
        A \in \FF_1 \cap \FF_2 \Leftrightarrow A \in \FF_1, \FF_2 \Rightarrow
        A^c \in \FF_1, \FF_2 \Leftrightarrow A^c \in \FF_1 \cap \FF_2
    \end{equation}

    Axiom \ref{eq:union}:
    Just applying \eqref{eq:altintersection} directly and using the fact that
    $\FF_1$ and $\FF_2$ is sigma algebras.
    \begin{equation}
        A, B \in \FF_1 \cap \FF_2 \Leftrightarrow A, B \in \FF_1, \FF_2 \Rightarrow
        A \cup B \in \FF_1, \FF_2 \Leftrightarrow A \cup B \in \FF_1 \cap \FF_2
    \end{equation}

    All sigma algebra axioms are satisfied and thus $\FF_1 \cap \FF_2$ is a
    sigma algebra $\qed$
%    \begin{identity}[De Morgan's rule]
%        \label{id:demorgan}
%        \begin{equation}
%            \label{eq:demorgan}
%            A \cap B = (A^c \cup B^c)^c
%        \end{equation}
%    \end{identity}

%    TODO introduce $\FF$ is this on the right level? corner casees? compare to
%    the general case %\Cap_cA_c (book?)
%    \begin{equation}
%        A, B \in \AA \stackrel{\eqref{eq:complement}}{\Longrightarrow}
%        A^c, B^c \in \AA \stackrel{\eqref{eq:union}}{\Longrightarrow}
%        (A^c \cup B^c) \in \AA \stackrel{\eqref{eq:demorgan}}{\Longrightarrow}
%        (A \cap B)^c \in \AA \stackrel{\eqref{eq:complement}}{\Longrightarrow}
%        A \cap B \in \AA \qed
%    \end{equation}

%    definition of algebra 1.3.1 (what is "sigma"?)
%    De morgan, "closure under union" and that A^c\in A
%    "if an algebra is finite it's also an sigma algebra"
%
%    infinite algebras?
%    1.3.2 definition of sigma algebra use these stuff on F1, F2 and derive F1
%    cap F2 stuff. this to make it as general as possible (do we need to use de
%    morgan?)
%    Lemma 1.3.10 does the exact same thing but in general, right?

\end{solution}
\pagebreak

%------------------------------------------------------------------------------

\begin{exercise}{2: 2.6.5.6} \textbf{Use Chen's Lemma} \\
    $X \in \distr{Po}{\lambda}$. Show that
    \begin{equation}
        \label{eq:expectedtopowern}
        \Expected[X^n] =
        \lambda \sum\limits_{k=0}^{n-1} \binom{n-1}{k}\Expected[X^k].
    \end{equation}
    \textit{Aid:} Use Chen's Lemma with suitable $H(x)$.
\end{exercise}
\begin{solution}
    %http://kth-primo.hosted.exlibrisgroup.com/primo_library/libweb/action/display.do?tabs=detailsTab&ct=display&fn=search&doc=KTH_LMSMILL.b10095585&indx=1&recIds=KTH_LMSMILL.b10095585&recIdxs=0&elementId=0&renderMode=poppedOut&displayMode=full&frbrVersion=&dscnt=0&frbg=&scp.scps=scope%3A%28KTH%29%2Cprimo_central_multiple_fe&tab=default_tab&dstmp=1412688608005&srt=rank&mode=Basic&&dum=true&vl%28freeText0%29=poisson%20approximation%20barbour&vid=KTH&gathStatIcon=true
    % TODO H bounded??
    \begin{lemma}[Chen's Lemma]
        \label{lemma:chen}  % TODO verify the correctness
        $X \in \distr{Po}{\lambda}$ and $H(x)$ is a bounded
        Borel function, then
        \begin{equation}
            \label{eq:chen}
            \Expected[XH(X)] = \lambda\Expected[H(X+1)].
        \end{equation}
    \end{lemma}
    \begin{identity}[Binomial identity]
        \label{id:binomial}
        \begin{equation}
            \label{eq:binomial}
            (x+y)^n = \sum\limits_{k=0}^n \binom{n}{k} x^ky^{n-k}
        \end{equation}
        \begin{equation}
            \label{eq:binomial1}
            (x+1)^n = \sum\limits_{k=0}^n \binom{n}{k} x^k
        \end{equation}
    \end{identity}

    We choose $H(X) = X^{n-1}$ to show \eqref{eq:expectedtopowern}.

    \begin{equation}
        \Expected[X^n] = \evaluatedat{\Expected[XH(X)]}{H(X)=X^{n-1}}
        \stackrel{\eqref{lemma:chen}}{=}
        \lambda \evaluatedat{\Expected[H(X+1)]}{H(X)=X^{n-1}} =
        \lambda \Expected[(X+1)^{n-1}]
    \end{equation}
    \begin{align}
        \lambda \Expected[(X+1)^{n-1}] & \stackrel{\eqref{eq:binomial1}}{=}
        \lambda \Expected
            \left[
                \sum\limits_{k=0}^{n-1} \binom{n-1}{k}X^k
            \right] =
        \left\{\Expected \text{linear operator}\right\} \nonumber \\ & =
        \lambda\sum\limits_{k=0}^{n-1} \binom{n-1}{k}\Expected[X^k] \qed
    \end{align}
\end{solution}
\pagebreak

%------------------------------------------------------------------------------

\begin{exercise}{3: 3.8.3.1}
    \textbf{Joint Distributions \& Conditional Expectations} \\
    Let $(X, Y)$ be a bivariate random variable, where $X$ is discrete and $Y$
    is continuous. $(X, Y)$ has a joint probability mass - and density function
    given by
    \begin{equation}
        f_{X,Y}(k, y) = \begin{cases}
            \partdev{P(X=k, Y\le y)}{y} =
            \lambda\frac{(\lambda y)^k}{k!}e^{-2\lambda y}
                    &,\, k\in\ZZ_{\ge 0},\, y\in [0, \infty) \\
            0       &.
        \end{cases}
    \end{equation}
    \begin{enumerate}
        \item Check that
            \begin{equation}
                \sum\limits_{k=0}^\infty \int\limits_0^\infty f_{X,Y}(k,y)dy =
                \int\limits_0^\infty \sum\limits_{k=0}^\infty f_{X,Y}(k,y)dy = 1
            \end{equation}

        \item \label{itm:b} Compute the mixed moment $\Expected[XY]$ defined as
            \begin{equation}
                \Expected[XY] =
                \sum\limits_{k=0}^\infty \int\limits_0^\infty kyf_{X,Y}(k,y)dy.
            \end{equation}
            \Answer $\frac{2}{\lambda}$

        \item Find the marginal p.m.f. of $X$.
            \Answer $X\in\distr{Ge}{\frac{1}{2}}$

        \item Compute the marginal density of $Y$ here defined as
            \begin{equation}
                f_Y(y) = \begin{cases}
                    \sum\limits_{k=0}^\infty f_{X,Y}(k,y) &, \, y\in[0,\infty) \\
                     0                                    &.
                \end{cases}
            \end{equation}
            \Answer $Y\in\distr{Exp}{\frac{1}{\lambda}}$

        \item Find
            \begin{equation}
                \gprob{\conditional{X}{Y}}{\conditional{k}{y}} =
                \Prob{\conditional{X=k}{Y=y}}, \, k\in\ZZ_{\ge 0}
            \end{equation}
            \Answer $\conditional{X}{Y} = y\in\distr{Po}{\lambda y}$.

        \item Compute $\Expected[\conditional{X}{Y=y}]$ and then
            $\Expected[XY]$ using double expectation.
            Compare your results with (b).

    \end{enumerate}
\end{exercise}
\begin{solution}
    First note that events with $0$ probability is dealt with via using certain domains
    when integrating and such, trivial to perform and trivial to append it back on but left out for
    brevity.\\
    Using the inverse series expansion of $e^{(.)}$ for $k$ as a
    function of $\lambda y$ we get the marginalized distribution
    \begin{equation}
        \label{eq:fY}
        f_Y(y) = \sum\limits_{k=0}^\infty
            \lambda \frac{(\lambda y)^k}{k!}e^{-2\lambda y} =
            \lambda e^{-\lambda y}
    \end{equation}

    Using Beta 7.5.40 with $\left\{a=2y, n=k\right\}$ we get
    \begin{align}
        \label{eq:fX}
        f_X(k) &= \int\limits_{y=-\infty}^\infty
            \lambda \frac{(\lambda y)^k}{k!}e^{-2\lambda y} \nonumber \\ &=
            \frac{\lambda^{k+1}}{k!} \int\limits_{y=0}^\infty y^k e^{-2\lambda
            y} \nonumber \\ &=
            \frac{\lambda^{k+1}}{k!} \left(\frac{k!}{(2\lambda)^{k+1}}\right)\nonumber \\ &=
            \frac{1}{2^{k+1}}
    \end{align}
    \begin{enumerate}
        % TODO show (c) and (d) and since those are distributions (a) follows
        \item To show that the order of integration/summation doesn't matter we can
            simply compute their values and see that they are the same and $=1$.

            Using Beta 7.5.40 with $\left\{a=\lambda, n=0\right\}$ we get
            \begin{equation}
                \int\limits_{y=0}^\infty f_Y(y) = \int\limits_{y=0}^\infty
                \lambda e^{-\lambda y}= \frac{\lambda}{\lambda^{0+1}}
                = 1
            \end{equation}

            Using geometric series for $\sum\frac{1}{1-x}$ with $x=\frac{1}{2}, |\frac{1}{2}|<1$
            \begin{equation}
                \sum\limits_{k=0}^\infty f_X(k) =
                \sum\limits_{k=0}^\infty \frac{1}{2^{k+1}} =
                \sum\limits_{k'=1}^\infty \frac{1}{2\cdot2^k} = \frac{2}{2} = 1
                \qed
            \end{equation}

        \item Using the same integration rules from Beta as before and Beta
            8.6.4 for the last sum

            \begin{align}
                \Expected[XY] & =
                \sum\limits_{k=0}^\infty \int\limits_0^\infty kyf_{X,Y}(k,y)dy
                \nonumber \\ & =
                \sum\frac{\lambda^{k+1}k^2}{k!} \int y^{k+1} e^{-(2\lambda)y}
                \nonumber \\ & =
                \sum\frac{\lambda^{k+1}k^2}{k!}
                \left(\frac{(k+1)!}{(2\lambda)^{k+2}}\right)
                \nonumber \\ & =
                \sum\frac{k^2\lambda^{k+1}}{(2\lambda)^{k+2}}
                \nonumber \\ & =
                \frac{1}{\lambda}\sum\frac{k^2}{2^{k+2}}
                \nonumber \\ & =
                \frac{1}{4\lambda}\sum\limits_{k=1}^\infty \frac{k^2}{2^k}
                \nonumber \\ & =
                \frac{1}{4\lambda}\left(1-1/2\right)^{-3}(1/2+1/2)
                \nonumber \\ & = \frac{2}{\lambda} \qed
            \end{align}

        \item Simple pattern matching on tabulated p.m.f. for
            $\distr{Ge}{\frac{1}{2}}$ on \eqref{eq:fX}
            \begin{equation}
                \frac{1}{2^{k+1}} =
                \evaluatedat{p(1-p)^k}{p=\frac{1}{2}} =
                \evaluatedat{\distr{Ge}{p}}{p=\frac{1}{2}} =
                \distr{Ge}{\frac{1}{2}}(k) \qed
            \end{equation}

        \item Simple pattern matching on tabulated p.m.f. for
            $\distr{Exp}{\frac{1}{\lambda}}$ on \eqref{eq:fY}
            \begin{equation}
                \lambda e^{-\lambda y} =
                \evaluatedat{
                    \frac{
                        e^{-x/a}
                    }{
                        a
                    }
                }{
                    a=\frac{1}{\lambda}
                } =
                \evaluatedat{\distr{Exp}{a}(x)}{a=\frac{1}{\lambda}} =
                \distr{Exp}{\frac{1}{\lambda}}(k) \qed
            \end{equation}

        \item From tabulation we have
            \begin{equation}
                \label{eq:podef}
                p_{\distr{Po}{\lambda y}}(k) = e^{-\lambda y} \frac{(\lambda
                y)^k}{k!}
            \end{equation}

            From the definition of conditional we have
            \begin{equation}
                p_{XY} = p_{\conditional{X}{Y}}p_{Y}
            \end{equation}
            This gives us when putting in the p.m.f.'s
            \begin{equation}
                p_{\conditional{X}{Y}} =
                \frac{p_{XY}}{p_{Y}} =
                \frac{
                    \lambda \frac{
                        (\lambda y)^ke^{-2\lambda y}
                    }{
                        k!
                    }
                }{
                    \lambda e^{-\lambda y}
                } =
                \frac{
                    (\lambda y)^k e^{-\lambda y}
                }{
                    k!
                } = e^{-\lambda y} \frac{(\lambda
                    y)^k}{k!} = 
                    p_{
                    \distr{Po}{\lambda y}}(k) 
            \end{equation}

            And since we have that distributions are equal we have that
            \begin{equation}
                \conditional{X}{Y=y} \in \distr{Po}{\lambda y} \qed
            \end{equation}

        \item

    \end{enumerate}
    %(a) %brute force from c/d ? discuss the difftrans sumint=intsum (when it's
        %valid that is)
    %(b)
    %(c) %beta 7.5.40
    %(d) %(almost) inverse taylor of e^x
    %(e)
    %(f)
\end{solution}
\pagebreak

%------------------------------------------------------------------------------

\begin{exercise}{4: 3.8.3.14} \textbf{Computations on a distribution} \\
    Let $(P,Y)$ be a bivariate r.v. such that
    \begin{equation}
        \conditional{Y}{P=p} \in \distr{Fs}{p},\quad
        f_P(p)=3p^2,\quad
        p\in [0, 1].
    \end{equation}
    Compute: $\Expected[Y]$, $\Variance[Y]$, $\CoVariance(P, Y)$ and the p.m.f.
    of $Y$.
    \Answer
    ${\Expected[Y]=\frac{3}{2}}$,
    ${\Variance[Y]=\frac{9}{4}}$,
    ${\CoVariance(P,Y)=-\frac{1}{8}}$, and
    $\gprob{Y}{k} = \frac{18}{(k+3)(k+2)(k+1)k},\, k \ge 1$.

\end{exercise}

\begin{solution}
    \begin{align}
        \gprob{Y}{k} &= \int\limits_{p=0}^1 \gprob{Y|P=p}{k} df_P(p) =
        3\int\limits_{p=0}^1 p^3 (1-p)^{k-1}dp = \left\{
            \begin{cases}
                q   & = 1-p \\
                dq  & = -dp
            \end{cases}
        \right\} \nonumber \\ & =
        3\int\limits_{q=0}^1 (1-q)^3 q^{k-1} dq =
        3\int\limits_{q=0}^1 (-q^3+3 q^2-3 q+1) q^{k-1} dq \nonumber \\ & =
        3\int\limits_{q=0}^1 -q^{k+2}+3q^{k+1}-3q^k+q^{k-1} dq \nonumber \\ & =
        3\left(-\frac{1}{k+3}+\frac{3}{k+2}-\frac{3}{k+1}+\frac{1}{k}\right) \nonumber \\ & =
        \frac{18}{(k+3)(k+2)(k+1)k}, k \ge 1
    \end{align} %\nonumber \\ & =

    Setting up Python with Sympy to leverage some of the algebra \\
    \begin{lstlisting}[frame=single]
        from sympy import *
        p, q, k = S('p k q'.split())
    \end{lstlisting}

    \begin{lstlisting}[frame=single]
        EY = summation(
            18/((k+3)*(k+2)*(k+1)),
            (k, 1, oo)
        )
        print(EY) # 3/2
    \end{lstlisting}
    From the definition of $\Expected$ we have
    \begin{align}
        \Expected[Y] & \eqdef \sum\limits_{k=1}^\infty k
        \frac{18}{(k+3)(k+2)(k+1)k} =
        \sum\limits_{k=1}^\infty \frac{18}{(k+3)(k+2)(k+1)} \nonumber \\ & =
        \frac{3}{2}
    \end{align}
    \begin{lstlisting}[frame=single]
        VarY = summation(
            18*(k-EY)**2/((k+3)*(k+2)*(k+1)),
            (k, 1, oo)
        )
        print(VarY) # 9/4
    \end{lstlisting}
    From the definition of $\Variance$ we have
    \begin{align}
        \Variance[Y] & \eqdef \sum\limits_{k=1}^\infty (k - \Expected[Y])^2
        \frac{18}{(k+3)(k+2)(k+1)k} =
        \frac{9}{4}
    \end{align}

    \begin{equation}
        \Expected[P] = \int\limits_{p=0}^1 3p^2dp = \frac{3}{4}
    \end{equation}
%    \begin{lstlisting}[frame=single]
%        EP = Rational(3, 4)
%        CovXY = integrate(
%            summation(
%                18*(k-EY)*(p-EP)*3*p**2/((k+3)*(k+2)*(k+1)),
%                (k, 1, oo),
%            ),
%            (p, 0, 1)
%        )
%        print(CovXY) # -1/8
%    \end{lstlisting}

    From the definition of \CoVariance  (and the trick using the $q = 1-p$)
    \begin{align}
        \CoVariance(P, Y) & \eqdef
            \int\sum (k - \Expected[Y])(p - \Expected[P]) p_{YP}(y, p)%\frac{18\cdot3p^2}{(k+3)(k+2)(k+1)k}
            \nonumber \\& =
            \int\sum (k - \Expected[Y])(p - \Expected[P]) p_{Y|P}(y, p)f_p(p)%\frac{18\cdot3p^2}{(k+3)(k+2)(k+1)k}
            \nonumber \\& =
             \int\sum (k - \Expected[Y])(p - \Expected[P]) p(1-p)^{k-1} \cdot
             3p^2
        = ... = % TODO
        -\frac{1}{8}
    \end{align}

\end{solution}
\pagebreak

%------------------------------------------------------------------------------

\begin{exercise}{5: 4.7.2.4} \textbf{Equidistribution} \\
    Let $\left\{{X_k}\right\}_{k=1}^n$ be independent and identically
    distributed. Furthermore $\left\{{a_k}\right\}_{k=1}^n$, $a_k \in \RR$. Set
    \begin{equation}
        Y_1 = \sum\limits_k a_k X_k
    \end{equation}
    and
    \begin{equation}
        Y_2 = \sum\limits_k a_{n-k+1} X_k.
    \end{equation}
    Show that
    \begin{equation}
        Y_1 \Equidistributed Y_2.
    \end{equation}
\end{exercise}
\begin{solution}
%    TODO needs background identies and theorems for all the steps
%    forall X exists unq varphiX
%    uniqness of varphiX ( => varphiX = varphiXk)
%    varphi sum
%    varphi scale
%    Xk is all identically distributed
%    product is reorderable due to (countable?) commutativity and associative of
%    multiplication
%    anything more?
%    TODO compare with FS8.3

    \begin{identity}[Permuted coefficients]
        \label{thm:permutedcoeffs}
        $\{X_k\}, X$ is independent and identically distributed.
        $\sigma$ is any permutation.
        \begin{equation}
            \varphi_{\left(\sum\limits_{k\in K}a_{\sigma(k)} X_k\right)}(t) =
            \prod\limits_{k\in K} \varphi_X (a_k t) =
            \varphi_{\left(\sum\limits_{k\in K}a_{k} X_k\right)}(t)
        \end{equation}
    \end{identity}
    \begin{proof}
        Holds directly from the identities
        \begin{equation}
            \varphi_{\sum X_k} = \prod \varphi_{X_k}
        \end{equation}
        \begin{equation}
            \varphi_{a X}(t) = \varphi_{X}(at)
        \end{equation}
        and the fact that one can reorder $\prod$ and uniqueness of $\varphi$.

    \end{proof}

    Using Theorem \ref{thm:permutedcoeffs} with
    $\sigma: k\rightarrow n-k+1$

    \begin{equation}
        \varphi_{Y_1} =
        \varphi_{\left(\sum\limits_k a_k X_k\right)} =
        \varphi_{\left(\sum\limits_k a_{n-k+1} X_k\right)} =
        \varphi_{Y_2}
    \end{equation}
    And with the uniqueness theorem we have
    \begin{equation}
        \left\{\varphi_{Y_1} = \varphi_{Y_2}\right\} \Rightarrow
        \left\{Y_1 \Equidistributed Y_2\right\} \qed
    \end{equation}
\end{solution}
\pagebreak

%------------------------------------------------------------------------------

\begin{exercise}{6: 5.8.3.11} \textbf{Laplace distribution} \\
    Let $\left\{{X_k}\right\}_{k=1}^n$ be independent and identically
    distributed with
    ${X_k \in\distr{$L$}{a},}\, {k\in[1,N_p],}\, {N_p\in \distr{Fs}{p}}$.
    $N_p$ is independent of the varibles $\left\{X_k\right\}$. We set
    \begin{equation}
        S_{N_p} = \sum\limits_{k=1}^{N_p}X_k.
    \end{equation}
    Show that $\sqrt{p}S_{N_p}\in\distr{$L$}{a}$.
\end{exercise}
\begin{solution}
    \begin{equation}
        \varphi_{\sqrt{p}S_{N_p}}(t) = \varphi_{S_{N_p}}(\sqrt{p}t)
    \end{equation}
    \begin{equation}
        \varphi_{S_{N_p}} = g_{\distr{Fs}{p}}(\varphi_{\distr{La}{a}}(t))
    \end{equation}
    From definition of generating function and using geometric series
    ($|t(1-p)|<1\Rightarrow |t|<1$)
    \begin{equation}
        g_{\distr{Fs}{p}}(t) = \sum\limits_{k=1}^\infty t^kp(1-p)^{k-1} =
        tp\sum\limits_{k=0}^\infty (t(1-p))^k = \frac{tp}{1-t(1-p)}
    \end{equation}

    From tabulation
    \begin{equation}
        \varphi_{\distr{La}{a}}(t) = \frac{1}{1+a^2t^2}
    \end{equation}
    \begin{equation}
        \varphi_{\sqrt{p}S_{N_p}} =
        g_{\distr{Fs}{p}}(\varphi_{\distr{La}{a}}(\sqrt{p}t))
    \end{equation}
    becomes a mess I couldn't untangle..

    but it should (and most if the answer is correct) by uniqueness theorem
    unfold to be
    \begin{equation}
        \varphi_{\sqrt{p}S_{N_p}} = \varphi_{\distr{La}{a}}(t)
    \end{equation}
    and thereby by uniqueness theorem having
    \begin{equation}
        \sqrt{p}S_{N_p}\in\distr{L}{a}.
    \end{equation}

    %problem book3.8.21
\end{solution}
\pagebreak

%------------------------------------------------------------------------------

\begin{exercise}{7: 7.6.1.1} \textbf{Mean square convergence}\\
    Assume $X_n, Y_n\in \distr{$L_2$}{\Omega,\FF,P}\, \forall n$ and
    \begin{equation}
        X_n \StackConverges{2} X, \quad
        Y_n \StackConverges{2} Y \quad
        \text{as}\quad n\rightarrow\infty
    \end{equation}
    Let $a, b \in \RR$. Show that
    \begin{equation}
        aX_n + bY_n\StackConverges{2} aX + bY\quad
        \text{as}\quad n\rightarrow\infty
    \end{equation}
    You should use the definition of mean square convergence and suitable
    properties of $\|X\|$ as defined in (LN 7.3).
\end{exercise}
\begin{solution}
    From the definition of mean square convergence:
    \begin{equation}
        \label{eq:xconv}
        X_n \StackConverges{2} X \Rightarrow
        \Expected[\|X_n-X\|^2] \rightarrow 0,
    \end{equation}
    \begin{equation}
        \label{eq:yconv}
        Y_n \StackConverges{2} Y \Rightarrow
        \Expected[\|Y_n-Y\|^2] \rightarrow 0.
    \end{equation}
    Via the parallelogram rule and \eqref{eq:xconv}, \eqref{eq:yconv}
    \begin{align}  % TODO explain parallelogram rule
        \label{eq:main7}
        \Expected[\|(aX_n+bY_n)-(aX+bY)\|^2] &=
        \Expected[\|a(X_n-X) + b(Y_n-Y)\|^2] \nonumber \\ &\le
        2a^2\Expected[\|X_n-X\|^2]+2b^2\Expected[\|Y_n-Y\|^2] \nonumber \\ &
        \rightarrow 0
    \end{align}
    Again from the definition of mean square convergence and
    \eqref{eq:main7} we get % TODO is this correct (use thm instead?)
    \begin{equation}
        \Expected[\|(aX_n+bY_n)-(aX+bY)\|^2] \rightarrow 0 \Rightarrow
        aX_n+bY_n \StackConverges{2} aX+bY \qed
    \end{equation}
    % TODO check 7.3 for inconsistencies
\end{solution}
\pagebreak

%-----------------------

\end{document}
