\documentclass[a4paper,twoside=false,abstract=false,numbers=noenddot,
titlepage=false,headings=small,parskip=half,version=last]{scrartcl}
\usepackage{lib/header}
\usepackage{lib/probability}
\begin{document}
\generateheader{Assignment 1}

\begin{exercise}{1: 1.12.3.10} {\bf Intersection of sigma algebras}   \\
    $\FF_1$ and $\FF_2$ are two sigma algebras of subsets of $\Omega$. Show
    that
    \begin{equation}
        \FF_1 \cap \FF_2
    \end{equation}
    is a sigma algebra of subsets of $\Omega$.
\end{exercise}
\begin{solution}
    \begin{definition}[Sigma algebra]
        A collection \AA\, of subsets of $\Omega$ is called a \textbf{sigma
        algebra} if it satisfies.
        \begin{enumerate}
            \item test
            \item test
        \end{enumerate}
    \end{definition}

%    definition of algebra 1.3.1 (what is "sigma"?)
%    De morgan, "closure under union" and that A^c\in A
%    "if an algebra is finite it's also an sigma algebra"
%
%    infinite algebras?
%    1.3.2 definition of sigma algebra use these stuff on F1, F2 and derive F1
%    cap F2 stuff. this to make it as general as possible (do we need to use de
%    morgan?)
%    Lemma 1.3.10 does the exact same thing but in general, right?

\end{solution}
\pagebreak

\begin{exercise}{2: 2.6.5.6} \textbf{Use Chen's Lemma} \\
    $X \in \distr{Po}{\lambda}$. Show that
    \begin{equation}
        \Expected[X^n] =
        \lambda \sum\limits_{k=0}^{n-1} \binom{n-1}{k}\Expected[X^k].
    \end{equation}
    \textit{Aid:} Use Chen's Lemma with suitable $H(x)$.
\end{exercise}
\begin{solution}
    %http://kth-primo.hosted.exlibrisgroup.com/primo_library/libweb/action/display.do?tabs=detailsTab&ct=display&fn=search&doc=KTH_LMSMILL.b10095585&indx=1&recIds=KTH_LMSMILL.b10095585&recIdxs=0&elementId=0&renderMode=poppedOut&displayMode=full&frbrVersion=&dscnt=0&frbg=&scp.scps=scope%3A%28KTH%29%2Cprimo_central_multiple_fe&tab=default_tab&dstmp=1412688608005&srt=rank&mode=Basic&&dum=true&vl%28freeText0%29=poisson%20approximation%20barbour&vid=KTH&gathStatIcon=true
    %H(X) = X^{n-1} is not bounded (or?) and the solution "comes close" but not
    %there
    \begin{lemma}[Chen's Lemma]
        \label{lemma:chen}  % TODO verify the correctness
        $X \in \distr{Po}{\lambda}$ and $H(x)$ is a bounded
        Borel function, then
        \begin{equation}
            \label{eq:chen}
            \Expected[XH(X)] = \lambda\Expected[H(X+1)].
        \end{equation}
    \end{lemma}
\end{solution}
\pagebreak

\begin{exercise}{3: 3.8.3.1}
    \textbf{Joint Distributions \& Conditional Expectations} \\
    Let $(X, Y)$ be a bivariate random variable, where $X$ is discrete and $Y$
    is continuous. $(X, Y)$ has a joint probability mass - and density function
    given by
    \begin{equation}
        f_{X,Y}(k, y) = \begin{cases}
            \partdev{P(X=k, Y\le y)}{y} =
            \lambda\frac{(\lambda y)^k}{k!}e^{-2\lambda y}
                    &,\, k\in\ZZ_{\ge 0},\, y\in [0, \infty) \\
            0       &.
        \end{cases}
    \end{equation}

    \begin{enumerate}
        \item Check that
            \begin{equation}
                \sum\limits_{k=0}^\infty \int\limits_0^\infty f_{X,Y}(k,y)dy =
                \int\limits_0^\infty \sum\limits_{k=0}^\infty f_{X,Y}(k,y)dy = 1
            \end{equation}

        \item Compute the mixed moment $\Expected[XY]$ defined as
            \begin{equation}
                \Expected[XY] =
                \sum\limits_{k=0}^\infty \int\limits_0^\infty f_{X,Y}(k,y)dy.
            \end{equation}
            \Answer $\frac{2}{\lambda}$

        \item Find the marginal p.m.f. of $X$.
            \Answer $X\in\distr{Ge}{\frac{1}{2}}$

        \item Compute the marginal density of $Y$ here defined as
            \begin{equation}
                f_Y(y) = \begin{cases}
                    \sum\limits_{k=0}^\infty f_{X,Y}(k,y) &, \, y\in[0,\infty) \\
                     0                                    &.
                \end{cases}
            \end{equation}
            \Answer $Y\in\distr{Exp}{\frac{1}{\lambda}}$

        \item Find
            \begin{equation}
                \gprob{\conditional{X}{Y}}{\conditional{k}{y}} =
                \Prob{\conditional{X=k}{Y=y}}, \, k\in\ZZ_{\ge 0}
            \end{equation}
            \Answer $\conditional{X}{Y} = y\in\distr{Po}{\lambda y}$.

        \item Compute $\Expected[\conditional{X}{Y=y}]$ and then
            $\Expected[XY]$ using double expectation.
            Compare your results with (b).

    \end{enumerate}
\end{exercise}
\begin{solution}
    \begin{enumerate}
        \item
        \item
        \item
        \item
        \item
        \item
        \item
    \end{enumerate}
    %(a) %brute force from c/d ? discuss the difftrans sumint=intsum (when it's
        %valid that is)
    %(b)
    %(c) %beta 7.5.40
    %(d) %(almost) inverse taylor of e^x
    %(e)
    %(f)
\end{solution}
\pagebreak

\begin{exercise}{4: 3.8.3.14} \textbf{Computations on a distribution} \\
    Let $(X,Y)$ be a bivariate r.v. such that
    \begin{equation}
        \conditional{Y}{X=x} \in \distr{Fs}{x},\quad
        f_X(x)=3x^2,\quad
        x\in [0, 1].
    \end{equation}
    Compute $\Expected[Y]$, $\Variance[Y]$, $\CoVariance(X, Y)$ and the p.m.f.
    of $Y$.
    \Answer
    ${\Expected[Y]=\frac{3}{2}}$,
    ${\Variance[Y]=\frac{9}{4}}$,
    ${\CoVariance(X,Y)=-\frac{1}{8}}$, and
    $\gprob{Y}{k} = \frac{18}{(k+3)(k+2)(k+1)k},\, k \ge 1$.

\end{exercise}
\begin{solution}

\end{solution}
\pagebreak

\begin{exercise}{5: 4.7.2.4} \textbf{Equidistribution} \\
    Let $\left\{{X_k}\right\}_{k=1}^n$ be independent and identically
    distributed. Furthermore $\left\{{a_k}\right\}_{k=1}^n$, $a_k \in \RR$. Set
    \begin{equation}
        Y_1 = \sum\limits_k a_k X_k
    \end{equation}
    and
    \begin{equation}
        Y_2 = \sum\limits_k a_{n-k+1} X_k.
    \end{equation}
    Show that
    \begin{equation}
        Y_1 \Equidistributed Y_2.
    \end{equation}
\end{exercise}
\begin{solution}
\end{solution}
\pagebreak

\begin{exercise}{6: 5.8.3.11} \textbf{Laplace distribution} \\
    Let $\left\{{X_k}\right\}_{k=1}^n$ be independent and identically
    distributed with
    ${X_k \in\distr{$L$}{a},}\, {k\in[1,N_p],}\, {N_p\in \distr{Fs}{p}}$.
    $N_p$ is independent of the varibles $\left\{X_k\right\}$. We set
    \begin{equation}
        S_{N_p} = \sum\limits_{k=1}^{N_p}X_k.
    \end{equation}
    Show that $\sqrt{p}S_{N_p}\in\distr{$L$}{a}$.
\end{exercise}
\begin{solution}
\end{solution}
\pagebreak

\begin{exercise}{7: 7.6.1.1} \textbf{Mean square convergence}\\
    Assume $X_n, Y_n\in \distr{$L_2$}{\Omega,\FF,P}\, \forall n$ and
    \begin{equation}
        X_n \StackConverges{2} X, \quad
        Y_n \StackConverges{2} Y \quad
        \text{as}\quad n\rightarrow\infty
    \end{equation}
    Let $a, b \in \RR$. Show that
    \begin{equation}
        aX_n + bY_n\StackConverges{2} aX + bY\quad
        \text{as}\quad n\rightarrow\infty
    \end{equation}
    You should use the definition of mean square convergence and suitable
    properties of $\|X\|$ as defined in (LN 7.3).
\end{exercise}
\begin{solution}
    % http://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_mean
    % listed as one of the properties for L_p
\end{solution}
\pagebreak

%-----------------------
\end{document}
