\documentclass[a4paper,twoside=false,abstract=false,numbers=noenddot,
titlepage=false,headings=small,parskip=half,version=last]{scrartcl}
\usepackage{lib/header}
\usepackage{lib/probability}
\begin{document}
\generateheader{Assignment 1}

%------------------------------------------------------------------------------

\begin{exercise}{1: 1.12.3.10} {\bf Intersection of sigma algebras}   \\
    $\FF_1$ and $\FF_2$ are two sigma algebras of subsets of $\Omega$. Show
    that
    \begin{equation}
        \FF_1 \cap \FF_2
    \end{equation}
    is a sigma algebra of subsets of $\Omega$.
\end{exercise}
\begin{solution}
    \begin{definition}[Sigma algebra]
        $\FF \subseteq \PP(\Omega)$ is called a \textbf{sigma
        algebra} if it satisfies.
        \begin{equation}
            \label{eq:nonempty}
            \Omega \in \FF
        \end{equation}
        \begin{equation}
            \label{eq:complement}
            A\in \FF \Rightarrow A^c \in \FF
        \end{equation}
        \begin{equation}
            \label{eq:union}
            A, B\in \FF \Rightarrow A\cup B \in \FF
        \end{equation}
        In other words, non-empty\eqref{eq:nonempty} and has closure under both
        complement\eqref{eq:complement} and union\eqref{eq:union}.
    \end{definition}

    Verify the sigma algebra axioms with the universe $\Omega$.\\

    By an alternative definition of intersection we have (which holds for any
    set)
    \begin{equation}
        \label{eq:altintersection}
        A \in \FF_1\wedge A \in \FF_2 = A \in \FF_1, \FF_2 \Leftrightarrow A \in \FF_1 \cap \FF_2
    \end{equation}

    Axiom \ref{eq:nonempty}:
    Just applying \eqref{eq:altintersection} directly and using the fact that
    $\FF_1$ and $\FF_2$ is sigma algebras.
    \begin{equation}
        \Omega \in \FF_1, \FF_2 \Rightarrow \Omega \in \FF_1 \cap \FF_2
    \end{equation}

    Axiom \ref{eq:complement}:
    Just applying \eqref{eq:altintersection} directly and using the fact that
    $\FF_1$ and $\FF_2$ is sigma algebras.
    \begin{equation}
        A \in \FF_1 \cap \FF_2 \Leftrightarrow A \in \FF_1, \FF_2 \Rightarrow
        A^c \in \FF_1, \FF_2 \Leftrightarrow A^c \in \FF_1 \cap \FF_2
    \end{equation}

    Axiom \ref{eq:union}:
    Just applying \eqref{eq:altintersection} directly and using the fact that
    $\FF_1$ and $\FF_2$ is sigma algebras.
    \begin{equation}
        A, B \in \FF_1 \cap \FF_2 \Leftrightarrow A, B \in \FF_1, \FF_2 \Rightarrow
        A \cup B \in \FF_1, \FF_2 \Leftrightarrow A \cup B \in \FF_1 \cap \FF_2
    \end{equation}

    All sigma algebra axioms are satisfied and thus $\FF_1 \cap \FF_2$ is a
    sigma algebra $\qed$
%    \begin{identity}[De Morgan's rule]
%        \label{id:demorgan}
%        \begin{equation}
%            \label{eq:demorgan}
%            A \cap B = (A^c \cup B^c)^c
%        \end{equation}
%    \end{identity}

%    TODO introduce $\FF$ is this on the right level? corner casees? compare to
%    the general case %\Cap_cA_c (book?)
%    \begin{equation}
%        A, B \in \AA \stackrel{\eqref{eq:complement}}{\Longrightarrow}
%        A^c, B^c \in \AA \stackrel{\eqref{eq:union}}{\Longrightarrow}
%        (A^c \cup B^c) \in \AA \stackrel{\eqref{eq:demorgan}}{\Longrightarrow}
%        (A \cap B)^c \in \AA \stackrel{\eqref{eq:complement}}{\Longrightarrow}
%        A \cap B \in \AA \qed
%    \end{equation}

%    definition of algebra 1.3.1 (what is "sigma"?)
%    De morgan, "closure under union" and that A^c\in A
%    "if an algebra is finite it's also an sigma algebra"
%
%    infinite algebras?
%    1.3.2 definition of sigma algebra use these stuff on F1, F2 and derive F1
%    cap F2 stuff. this to make it as general as possible (do we need to use de
%    morgan?)
%    Lemma 1.3.10 does the exact same thing but in general, right?

\end{solution}
\pagebreak

%------------------------------------------------------------------------------

\begin{exercise}{2: 2.6.5.6} \textbf{Use Chen's Lemma} \\
    $X \in \distr{Po}{\lambda}$. Show that
    \begin{equation}
        \label{eq:expectedtopowern}
        \Expected[X^n] =
        \lambda \sum\limits_{k=0}^{n-1} \binom{n-1}{k}\Expected[X^k].
    \end{equation}
    \textit{Aid:} Use Chen's Lemma with suitable $H(x)$.
\end{exercise}
\begin{solution}
    %http://kth-primo.hosted.exlibrisgroup.com/primo_library/libweb/action/display.do?tabs=detailsTab&ct=display&fn=search&doc=KTH_LMSMILL.b10095585&indx=1&recIds=KTH_LMSMILL.b10095585&recIdxs=0&elementId=0&renderMode=poppedOut&displayMode=full&frbrVersion=&dscnt=0&frbg=&scp.scps=scope%3A%28KTH%29%2Cprimo_central_multiple_fe&tab=default_tab&dstmp=1412688608005&srt=rank&mode=Basic&&dum=true&vl%28freeText0%29=poisson%20approximation%20barbour&vid=KTH&gathStatIcon=true
    % TODO H bounded??
    \begin{lemma}[Chen's Lemma]
        \label{lemma:chen}  % TODO verify the correctness
        $X \in \distr{Po}{\lambda}$ and $H(x)$ is a bounded
        Borel function, then
        \begin{equation}
            \label{eq:chen}
            \Expected[XH(X)] = \lambda\Expected[H(X+1)].
        \end{equation}
    \end{lemma}
    \begin{identity}[Binomial identity]
        \label{id:binomial}
        \begin{equation}
            \label{eq:binomial}
            (x+y)^n = \sum\limits_{k=0}^n \binom{n}{k} x^ky^{n-k}
        \end{equation}
        \begin{equation}
            \label{eq:binomial1}
            (x+1)^n = \sum\limits_{k=0}^n \binom{n}{k} x^k
        \end{equation}
    \end{identity}

    We choose $H(X) = X^{n-1}$ to show \eqref{eq:expectedtopowern}.

    \begin{equation}
        \Expected[X^n] = \evaluatedat{\Expected[XH(X)]}{H(X)=X^{n-1}}
        \stackrel{\eqref{lemma:chen}}{=}
        \lambda \evaluatedat{\Expected[H(X+1)]}{H(X)=X^{n-1}} =
        \lambda \Expected[(X+1)^{n-1}]
    \end{equation}
    \begin{equation}
        \lambda \Expected[(X+1)^{n-1}] \stackrel{\eqref{eq:binomial1}}{=}
        \lambda \Expected
            \left[
                \sum\limits_{k=0}^{n-1} \binom{n-1}{k}X^k
            \right] =
        \left\{\Expected \text{linear operator}\right\} =
        \lambda\sum\limits_{k=0}^{n-1} \binom{n-1}{k}\Expected[X^k] \qed
    \end{equation}
\end{solution}
\pagebreak

%------------------------------------------------------------------------------

\begin{exercise}{3: 3.8.3.1}
    \textbf{Joint Distributions \& Conditional Expectations} \\
    Let $(X, Y)$ be a bivariate random variable, where $X$ is discrete and $Y$
    is continuous. $(X, Y)$ has a joint probability mass - and density function
    given by
    \begin{equation}
        f_{X,Y}(k, y) = \begin{cases}
            \partdev{P(X=k, Y\le y)}{y} =
            \lambda\frac{(\lambda y)^k}{k!}e^{-2\lambda y}
                    &,\, k\in\ZZ_{\ge 0},\, y\in [0, \infty) \\
            0       &.
        \end{cases}
    \end{equation}
    \begin{enumerate}
        \item Check that
            \begin{equation}
                \sum\limits_{k=0}^\infty \int\limits_0^\infty f_{X,Y}(k,y)dy =
                \int\limits_0^\infty \sum\limits_{k=0}^\infty f_{X,Y}(k,y)dy = 1
            \end{equation}

        \item \label{itm:b} Compute the mixed moment $\Expected[XY]$ defined as
            \begin{equation}
                \Expected[XY] =
                \sum\limits_{k=0}^\infty \int\limits_0^\infty kyf_{X,Y}(k,y)dy.
            \end{equation}
            \Answer $\frac{2}{\lambda}$

        \item Find the marginal p.m.f. of $X$.
            \Answer $X\in\distr{Ge}{\frac{1}{2}}$

        \item Compute the marginal density of $Y$ here defined as
            \begin{equation}
                f_Y(y) = \begin{cases}
                    \sum\limits_{k=0}^\infty f_{X,Y}(k,y) &, \, y\in[0,\infty) \\
                     0                                    &.
                \end{cases}
            \end{equation}
            \Answer $Y\in\distr{Exp}{\frac{1}{\lambda}}$

        \item Find
            \begin{equation}
                \gprob{\conditional{X}{Y}}{\conditional{k}{y}} =
                \Prob{\conditional{X=k}{Y=y}}, \, k\in\ZZ_{\ge 0}
            \end{equation}
            \Answer $\conditional{X}{Y} = y\in\distr{Po}{\lambda y}$.

        \item Compute $\Expected[\conditional{X}{Y=y}]$ and then
            $\Expected[XY]$ using double expectation.
            Compare your results with (b).

    \end{enumerate}
\end{exercise}
\begin{solution}
    \begin{enumerate}
        % TODO show (c) and (d) and since those are distributions (a) follows
        \item
        \item
        \item
        \item
        \item
        \item
        \item
    \end{enumerate}
    %(a) %brute force from c/d ? discuss the difftrans sumint=intsum (when it's
        %valid that is)
    %(b)
    %(c) %beta 7.5.40
    %(d) %(almost) inverse taylor of e^x
    %(e)
    %(f)
\end{solution}
\pagebreak

%------------------------------------------------------------------------------

\begin{exercise}{4: 3.8.3.14} \textbf{Computations on a distribution} \\
    Let $(P,Y)$ be a bivariate r.v. such that
    \begin{equation}
        \conditional{Y}{P=p} \in \distr{Fs}{p},\quad
        f_P(p)=3p^2,\quad
        p\in [0, 1].
    \end{equation}
    Compute: $\Expected[Y]$, $\Variance[Y]$, $\CoVariance(P, Y)$ and the p.m.f.
    of $Y$.
    \Answer
    ${\Expected[Y]=\frac{3}{2}}$,
    ${\Variance[Y]=\frac{9}{4}}$,
    ${\CoVariance(P,Y)=-\frac{1}{8}}$, and
    $\gprob{Y}{k} = \frac{18}{(k+3)(k+2)(k+1)k},\, k \ge 1$.

\end{exercise}
\begin{solution}

    \begin{align}
        \gprob{Y}{k} &= \int\limits_{p=0}^1 \gprob{Y|P=p}{k} df_P(p) =
        3\int\limits_{p=0}^1 p^3 (1-p)^{k-1}dp = \left\{
            \begin{cases}
                q   & = 1-p \\
                dq  & = -dp
            \end{cases}
        \right\} \nonumber \\ & =
        3\int\limits_{q=0}^1 (1-q)^3 q^{k-1} dq =
        3\int\limits_{q=0}^1 (-q^3+3 q^2-3 q+1) q^{k-1} dq \nonumber \\ & =
        3\int\limits_{q=0}^1 -q^{k+2}+3q^{k+1}-3q^k+q^{k-1} dq \nonumber \\ & =
        3\left(-\frac{1}{k+3}+\frac{3}{k+2}-\frac{3}{k+1}+\frac{1}{k}\right) \nonumber \\ & =
        \frac{18}{(k+3)(k+2)(k+1)k}, k \ge 1
    \end{align} %\nonumber \\ & =

    \begin{align}
        \Expected[Y] & \eqdef \sum\limits_{k=1}^\infty k
        \frac{18}{(k+3)(k+2)(k+1)k} =
        \sum\limits_{k=1}^\infty \frac{18}{(k+3)(k+2)(k+1)} \nonumber \\ & =
        \frac{3}{2}
    \end{align}
    \begin{align}
        \Variance[Y] & \eqdef \sum\limits_{k=1}^\infty (k - \Expected[Y])^2
        \frac{18}{(k+3)(k+2)(k+1)k} =
        \frac{9}{4}
    \end{align}
    \begin{equation}
        \Expected[P] = \int\limits_{p=0}^1 3p^2dp = \frac{3}{4}
    \end{equation}
    \begin{align}
        \CoVariance(P, Y) & \eqdef \int\sum
            (k - \Expected[Y])(p - \Expected[P])
        ... =
        -\frac{1}{8}
    \end{align}



\end{solution}
\pagebreak

%------------------------------------------------------------------------------

\begin{exercise}{5: 4.7.2.4} \textbf{Equidistribution} \\
    Let $\left\{{X_k}\right\}_{k=1}^n$ be independent and identically
    distributed. Furthermore $\left\{{a_k}\right\}_{k=1}^n$, $a_k \in \RR$. Set
    \begin{equation}
        Y_1 = \sum\limits_k a_k X_k
    \end{equation}
    and
    \begin{equation}
        Y_2 = \sum\limits_k a_{n-k+1} X_k.
    \end{equation}
    Show that
    \begin{equation}
        Y_1 \Equidistributed Y_2.
    \end{equation}
\end{exercise}
\begin{solution}
    TODO needs background identies and theorems for all the steps
    forall X exists unq varphiX
    uniqness of varphiX ( => varphiX = varphiXk)
    varphi sum
    varphi scale
    Xk is all identically distributed
    product is reorderable due to (countable?) commutativity and associative of
    multiplication
    anything more?
    TODO compare with FS8.3

    \begin{identity}[Permuted coefficients]
        \label{thm:permutedcoeffs}
        $\{X_k\}, X$ is independent and identically distributed.
        $\sigma$ is any permutation.
        \begin{equation}
            \varphi_{\left(\sum\limits_{k\in K}a_{\sigma(k)} X_k\right)}(t) =
            \prod\limits_{k\in K} \varphi_X (a_k t) =
            \varphi_{\left(\sum\limits_{k\in K}a_{k} X_k\right)}(t)
        \end{equation}
    \end{identity}
    \begin{proof}
        \begin{equation}
            \varphi TODO
            %\prod\limits_{k\in K} \varphi_{X_k}(a_k t) =
            % TODO remember than only the coeffs are reordered sigma(K) is thus not
            % the right thing to write but a_\sigma(k)X_k
        \end{equation}
    \end{proof}

    Using Theorem \ref{thm:permutedcoeffs} with
    $\sigma: k\rightarrow n-k+1$

    \begin{equation}
        \varphi_{Y_1} =
        \varphi_{\left(\sum\limits_k a_k X_k\right)} =
        \varphi_{\left(\sum\limits_k a_{n-k+1} X_k\right)} =
        \varphi_{Y_2}
    \end{equation}
    And with uniqueness theorem of blablabla we have
    \begin{equation}
        \left\{\varphi_{Y_1} = \varphi_{Y_2}\right\} \Rightarrow
        \left\{Y_1 \Equidistributed Y_2\right\} \qed
    \end{equation}
\end{solution}
\pagebreak

%------------------------------------------------------------------------------

\begin{exercise}{6: 5.8.3.11} \textbf{Laplace distribution} \\
    Let $\left\{{X_k}\right\}_{k=1}^n$ be independent and identically
    distributed with
    ${X_k \in\distr{$L$}{a},}\, {k\in[1,N_p],}\, {N_p\in \distr{Fs}{p}}$.
    $N_p$ is independent of the varibles $\left\{X_k\right\}$. We set
    \begin{equation}
        S_{N_p} = \sum\limits_{k=1}^{N_p}X_k.
    \end{equation}
    Show that $\sqrt{p}S_{N_p}\in\distr{$L$}{a}$.
\end{exercise}
\begin{solution}
    %problem book3.8.21
\end{solution}
\pagebreak

%------------------------------------------------------------------------------

\begin{exercise}{7: 7.6.1.1} \textbf{Mean square convergence}\\
    Assume $X_n, Y_n\in \distr{$L_2$}{\Omega,\FF,P}\, \forall n$ and
    \begin{equation}
        X_n \StackConverges{2} X, \quad
        Y_n \StackConverges{2} Y \quad
        \text{as}\quad n\rightarrow\infty
    \end{equation}
    Let $a, b \in \RR$. Show that
    \begin{equation}
        aX_n + bY_n\StackConverges{2} aX + bY\quad
        \text{as}\quad n\rightarrow\infty
    \end{equation}
    You should use the definition of mean square convergence and suitable
    properties of $\|X\|$ as defined in (LN 7.3).
\end{exercise}
\begin{solution}
    From the definition of mean square convergence:
    \begin{equation}
        \label{eq:xconv}
        X_n \StackConverges{2} X \Rightarrow
        \Expected[\|X_n-X\|^2] \rightarrow 0,
    \end{equation}
    \begin{equation}
        \label{eq:yconv}
        Y_n \StackConverges{2} Y \Rightarrow
        \Expected[\|Y_n-Y\|^2] \rightarrow 0.
    \end{equation}
    Via the parallelogram rule and \eqref{eq:xconv}, \eqref{eq:yconv}
    \begin{align}  % TODO explain parallelogram rule
        \label{eq:main7}
        \Expected[\|(aX_n+bY_n)-(aX+bY)\|^2] &=
        \Expected[\|a(X_n-X) + b(Y_n-Y)\|^2] \nonumber \\ &\le
        2a^2\Expected[\|X_n-X\|^2]+2b^2\Expected[\|Y_n-Y\|^2] \nonumber \\ &
        \rightarrow 0
    \end{align}
    Again from the definition of mean square convergence and
    \eqref{eq:main7} we get % TODO is this correct (use thm instead?)
    \begin{equation}
        \Expected[\|(aX_n+bY_n)-(aX+bY)\|^2] \rightarrow 0 \Rightarrow
        aX_n+bY_n \StackConverges{2} aX+bY \qed
    \end{equation}
    % TODO check 7.3 for inconsistencies
\end{solution}
\pagebreak

%-----------------------

\end{document}
